dataset_params:
  # parent directory of the dataset
  # local path: 
  #            1) '/home/angelo/Documents/Echocardiography/echocardiography/regression' with 'DATA'
  #            2) '/media/angelo/OS/Users/lasal/Desktop' with 'DATA_h' from settember 2024
  # one drive: '...'
  # cineca: '/leonardo_work/IscrC_Med-LMGM/Angelo/echo_data/regression'
  parent_dir: '/media/angelo/OS/Users/lasal/Desktop'   
  im_path: 'DATA_h'        # 'DATA_h' from settember 2024, compatible with cineca
  split: 'train'
  split_val: 'val'
  split_test: 'test'
  im_channels : 1
  im_size_h : 240  # 256
  im_size_w : 320  # 256
  name: 'eco'
  dataset_batch: ['Batch1','Batch3', 'Batch4']  # dataset diffusion task, list of batch to be used 
  phase: 'diastole'
  
  ## trained model path is parent path where I collect the trained REGRESSION models
  # local path: '/home/angelo/Documents/Echocardiography/echocardiography/regression/TRAINED_MODEL'
  # cineca: '/leonardo_work/IscrC_Med-LMGM/Angelo/trained_model/regression' 
  parent_dir_regression: '/home/angelo/Documents/Echocardiography/echocardiography/regression/TRAINED_MODEL'  # path to the trained model
  trial: 'trial_2'                    # trial for regression task - used for semantic conditioning and clinical validation
  dataset_batch_regression: 'Batch2'  # dataset batch for regression task, it nust be different from dataset_batch

diffusion_params:
  num_timesteps : 1000
  beta_start : 0.0015
  beta_end : 0.0195

ldm_params:
  down_channels: [ 128, 256, 256, 256]
  mid_channels: [ 256, 256 ]
  down_sample: [ True, True, True]    ## [True, True, True] for the DDPM model, [False, True, False] for the LDM model
  attn_down : [True, True, True]
  time_emb_dim: 256
  norm_channels: 32
  num_heads: 8
  conv_out_channels : 128
  num_down_layers : 2
  num_mid_layers : 2
  num_up_layers : 2
  condition_config:
    condition_types: [ 'image' ]      # could be also ['class', 'text', 'image' ]: PUT THE DEFAULT VALUE AS 'image'
    text_condition_config:
      text_embed_model: 'clip'
      train_text_embed_model: False
      text_embed_dim: 512
      cond_drop_prob: 0.1
    image_condition_config:                 ## this is for the image conditioning - seg/heatmaps stacked on the input
      image_condition_input_channels: 6     # total number of spatial channels in the conditional heatmaps
      image_condition_output_channels: 3
      image_condition_h : 240
      image_condition_w : 320
      cond_drop_prob: 0.1
    class_condition_config :     ## this is for the class conditioning of different type of hypertrophy 
      num_classes : 4           # numebr of classes, for now it is 4
      cond_drop_prob : 0.1       # probability of dropping class labels

autoencoder_params:
  z_channels: 3
  codebook_size : 20
  down_channels : [64, 128, 256, 256]
  mid_channels : [256, 256]
  down_sample : [True, True, True]
  attn_down : [False, False, False]
  norm_channels: 32
  num_heads: 16
  num_down_layers : 2
  num_mid_layers : 2
  num_up_layers : 2
  condition_config:
    condition_types: [ 'class' ]      # could be also ['class', 'text', 'image' ]: PUT THE DEFAULT VALUE AS 'image'
    text_condition_config:
      text_embed_model: 'clip'
      train_text_embed_model: False
      text_embed_dim: 512
      cond_drop_prob: 0.1
    image_condition_config:                 ## this is for the image conditioning - seg/heatmaps stacked on the input
      image_condition_input_channels: 6     # total number of spatial channels in the conditional heatmaps
      image_condition_output_channels: 3
      image_condition_h : 256
      image_condition_w : 256
      cond_drop_prob: 0.1
    class_condition_config :     ## this is for the class conditioning of different type of hypertrophy 
      num_classes : 4           # numebr of classes, for now it is 4
      cond_drop_prob : 0.1       # probability of dropping class labels


train_params:
  task_name: 'eco'
  seed : 1111
  ldm_batch_size: 8
  ldm_batch_size_sample: 32
  autoencoder_batch_size: 8   # 2: local for shape (240,320) - 4 or 8: cineca for shape (240, 320)
  disc_start: 2
  disc_weight: 0.5
  codebook_weight: 1
  commitment_beta: 0.2
  perceptual_weight: 1
  kl_weight: 0.000005
  ldm_epochs : 150
  autoencoder_epochs : 100
  num_samples : 25
  num_grid_rows : 5
  ldm_lr: 0.00001
  autoencoder_lr: 0.0001
  autoencoder_acc_steps : 1
  autoencoder_img_save_steps : 8
  save_frequency : 10