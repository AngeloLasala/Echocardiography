Guideline to use in a clever way this repository on LEONARDO cluster.
The main idea is to use this as a proper repository and use each code as a module that take as 
input the proper information as flags.

The first informatio is the tree structure of the project. ALL the data and the code are in the 
provided $WORK directory of cineca. 
$ cd $WORK

the currect tree strucuture is:

Angelo
├── Echocardiography  ## repository with ALL the codes about the echocardiography
|    └── echocardiography
├── echo_data         ## folder with ALL the data about the echocardiography
|   └── DATA
├── venv              ## folder with ALL the virtual envs
|    └── eco
├── trained_model     ## folder to save ALL the file created from the training
|    ├── diffusion
|    └── regression


1. ACTIVATE THE VENV
To activate the eco env, in 'Angelo':
$ source venv/eco/bin/activate
NOTE: keep in mind that when you create a new file with new required packages, you have to reinstall all the repository
and update the venv. to do so, use the file setup.py and the requirements.txt

2. RUN A PYTHON CODE
site: https://wiki.u-gov.it/confluence/display/SCAIUS/UG2.6.1%3A+How+to+submit+the+job+-+Batch+Scheduler+SLURM
The easy way to run a python code in this tree is to leverege the structure of the repository and
run ANY KIND of python code as a module from the 'Echocardiography' directory. So, in the 'Echocardiography' run:
$ python -m path.of.the.code --flag
$ python -m echocardiography.diffusion.tools.sample_cond_ldm --data eco_image_cond (this is a simple example)

And use this type of command line with the srun command:
$ srun -N1 -n1 --gres=gpu:1 -p boost_usr_prod -A IscrC_Med-LMGM --time= 0:30 --output=file.out python -m path.of.the.code --flag
data :    --data_path '/leonardo_work/IscrC_Med-LMGM/Angelo/echo_data/regression/DATA'
results:  --save_dir '/leonardo_work/IscrC_Med-LMGM/Angelo/trained_model'

the best alternative to avoid the problem of connection loss is using the sbatch file. It contains the same informationo of srun
here and example.
example.sh
"""
#!/bin/bash
#SBATCH --nodes=1                    # 1 node
#SBATCH --ntasks=1
#SBATCH --gres=gpu:4
#SBATCH --ntasks-per-node=4          # 4 tasks per node
#SBATCH --time=1:00:00               # time limits: 1 hour
#SBATCH --partition=boost_usr_prod   # partition name
#SBATCH --error=error_file.err       # standard error file
#SBATCH --output=file_print.out      # standard output file
#SBATCH --account=IscrC_Med-LMGM     # account name
#SBATCH --qos=<qos_name>             # quality of service (!!NOT MANDATORY!!)

python my_code.py --flags
"""
NOTE: for the error file and the output file use the parent working directory 
      regression: /leonardo_work/IscrC_Med-LMGM/Angelo/Echocardiography/echocardiography/regression/utils/sbatch_files


submit a job: sbatch example.sh -> Submitted batch job 7366420
check the status: squeue --user alasala0
check the memory and computational time: cindata saldo (-b)

3. RETRIEVE THE FILE IN LOCAL
Once you trained, you have to push back the output folder locally, for example as a backup 
All the trained model (diffusion and regression) are in this folder that mimic the structure of 'trained_model folder' of 
local repository

cluster folder: '/leonardo_work/IscrC_Med-LMGM/Angelo/trained_model'
tree
../trained_model
    ├── diffusion
    │   ├── eco
    │   │   ├── trial_1
    │   │   │   ├── vae
    │   │   │   └── cond_ldm_1
    │   ├── train
    └── regression
    │   │   ├── Batch2
    │   │   │   ├── diastole
    |   |   |   |   ├── trial_1
    │   │   │   └── sistole (???)

in a terminal without ssh connection
scp -r alasala0@login.leonardo.cineca.it:<PERCORSO_REMOTO> <PERCORSO_LOCALE>

example for single trial
percorso_remoto: '/leonardo_work/IscrC_Med-LMGM/Angelo/trained_model/regression/Batch2/diastole/trial_1'
percorso_locale: '/media/angelo/OS/Users/lasal/OneDrive - Scuola Superiore Sant'Anna/PhD_notes/Echocardiografy/trained_model/regression/Batch2'
